<!doctype html><html class=no-js lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Apache Kafka CCDAK Exam Notes - Coding N Concepts</title><script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script><meta name=description content="Study guide for Kafka Certification CCDAK (Certified Developer for Apache Kafka) and CCOAK (Certified Operator for Apache Kafka) preparation with sample exam questions."><meta property="og:title" content="Apache Kafka CCDAK Exam Notes"><meta property="og:description" content="Study guide for Kafka Certification CCDAK (Certified Developer for Apache Kafka) and CCOAK (Certified Operator for Apache Kafka) preparation with sample exam questions."><meta property="og:type" content="article"><meta property="og:url" content="https://codingnconcepts.com/post/apache-kafka-ccdak-exam-notes/"><meta property="og:image" content="https://codingnconcepts.com/img/post/ccdak-exam-notes-banner.png"><meta property="article:published_time" content="2019-06-13T00:00:00+00:00"><meta property="article:modified_time" content="2020-12-24T00:00:00+00:00"><meta property="og:site_name" content="CodingNConcepts"><meta property="article:author" content="https://www.facebook.com/codingnconcepts"><meta property="article:publisher" content="https://www.facebook.com/codingnconcepts"><meta property="article:section" content="post"><meta property="article:tag" content="Confluent Kafka CCDAK"><meta property="article:tag" content="Popular Posts"><meta property="fb:admins" content="177784346770196"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://codingnconcepts.com/img/post/ccdak-exam-notes-banner.png"><meta name=twitter:title content="Apache Kafka CCDAK Exam Notes"><meta name=twitter:description content="Study guide for Kafka Certification CCDAK (Certified Developer for Apache Kafka) and CCOAK (Certified Operator for Apache Kafka) preparation with sample exam questions."><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700|Rowdies"><link rel=stylesheet href=/css/style.css><link rel="shortcut icon" href=/favicon.ico><link rel=canonical href=https://codingnconcepts.com/post/apache-kafka-ccdak-exam-notes/><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-160024498-1');</script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-160024498-1"></script><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"post","name":"Apache Kafka CCDAK Exam Notes","headline":"Apache Kafka CCDAK Exam Notes","alternativeHeadline":null,"description":"Study guide for Kafka Certification CCDAK (Certified Developer for Apache Kafka) and CCOAK (Certified Operator for Apache Kafka) preparation with sample exam questions.","inLanguage":"en-us","isFamilyFriendly":true,"mainEntityOfPage":{"@type":"WebPage","@id":"https://codingnconcepts.com/post/apache-kafka-ccdak-exam-notes/"},"author":{"@type":"Person","name":"Ashish Lahoti"},"creator":{"@type":"Person","name":"Ashish Lahoti"},"accountablePerson":{"@type":"Person","name":"Ashish Lahoti"},"copyrightHolder":"Coding N Concepts","copyrightYear":"2019","dateCreated":"2019-06-13T00:00:00.00Z","datePublished":"2019-06-13T00:00:00.00Z","dateModified":"2020-12-24T00:00:00.00Z","publisher":{"@type":"Organization","name":"Coding N Concepts","url":"https://codingnconcepts.com/","logo":{"@type":"ImageObject","url":"https://codingnconcepts.com/logo-amp.png","width":600,"height":60}},"image":["https://codingnconcepts.com/img/post/ccdak-exam-notes-banner.png"],"url":"https://codingnconcepts.com/post/apache-kafka-ccdak-exam-notes/","wordCount":4447,"genre":["Confluent Kafka CCDAK","Popular Posts"],"keywords":["confluent certified developer for apache kafka","kafka certification sample questions"]}</script></head><body class=body><div class="container container--outer"><header class=header><div class=container><div class=logo><a class=logo__link href=/ title="Coding N Concepts" rel=home><div class=logo__title><img src=/favicon-32x32.png alt="Apache Kafka CCDAK Exam Notes"> Coding N Concepts</div><div class=logo__tagline>Ashish Lahoti's Technical Blog</div></a></div><nav class=menu><button class=menu__btn aria-haspopup=true aria-expanded=false tabindex=0>
<span class=menu__btn-title tabindex=-1>Menu</span></button><ul class=menu__list><li class=menu__item><a class=menu__link href=/><span class=menu__text>Home</span></a></li><li class=menu__item><a class=menu__link href=/tags/popular-posts><span class=menu__text>Popular</span></a></li><li class="menu__item menu__dropdown"><a class=menu__link href=/java/><span class=menu__text>Java</span>
<label class=drop-icon for=Java>▾</label></a>
<input type=checkbox id=Java><ul class=submenu__list><li class=menu__item><a class=menu__link href=/tags/java-core/><span class=menu__text>Core Java</span></a></li><li class=menu__item><a class=menu__link href=/tags/java-collection/><span class=menu__text>Collections</span></a></li><li class=menu__item><a class=menu__link href=/tags/java-design-pattern/><span class=menu__text>Design Patterns</span></a></li><li class=menu__item><a class=menu__link href=/tags/java-recursive/><span class=menu__text>Recursive</span></a></li><li class=menu__item><a class=menu__link href=/tags/java-streams/><span class=menu__text>Streams</span></a></li><li class=menu__item><a class=menu__link href=/tags/java-string/><span class=menu__text>String</span></a></li><li class=menu__item><a class=menu__link href=/tags/java-numbers/><span class=menu__text>Numbers</span></a></li><li class=menu__item><a class=menu__link href=/tags/java-threads/><span class=menu__text>Threads</span></a></li></ul></li><li class="menu__item menu__dropdown"><a class=menu__link href=/javascript/><span class=menu__text>JavaScript</span>
<label class=drop-icon for=JavaScript>▾</label></a>
<input type=checkbox id=JavaScript><ul class=submenu__list><li class=menu__item><a class=menu__link href=/tags/javascript-core/><span class=menu__text>JS Core</span></a></li><li class=menu__item><a class=menu__link href=/tags/javascript-object/><span class=menu__text>JS Object</span></a></li><li class=menu__item><a class=menu__link href=/tags/javascript-string/><span class=menu__text>JS String</span></a></li><li class=menu__item><a class=menu__link href=/tags/javascript-array/><span class=menu__text>JS Array</span></a></li><li class=menu__item><a class=menu__link href=/tags/javascript-console/><span class=menu__text>JS Console</span></a></li><li class=menu__item><a class=menu__link href=/tags/javascript-es6/><span class=menu__text>ES6</span></a></li></ul></li><li class="menu__item menu__dropdown"><a class=menu__link href=/spring-boot/><span class=menu__text>Spring Boot</span>
<label class=drop-icon for="Spring Boot">▾</label></a>
<input type=checkbox id="Spring Boot"><ul class=submenu__list><li class=menu__item><a class=menu__link href=/tags/spring-boot-basics/><span class=menu__text>Spring Boot Basics</span></a></li><li class=menu__item><a class=menu__link href=/tags/spring-boot-rest/><span class=menu__text>Spring Boot REST</span></a></li><li class=menu__item><a class=menu__link href=/tags/spring-boot-kafka/><span class=menu__text>Spring Boot KAFKA</span></a></li></ul></li><li class="menu__item menu__dropdown"><a class=menu__link href=/puzzle/><span class=menu__text>Puzzles</span>
<label class=drop-icon for=Puzzles>▾</label></a>
<input type=checkbox id=Puzzles><ul class=submenu__list><li class=menu__item><a class=menu__link href=/tags/maths-puzzle/><span class=menu__text>Maths Puzzles</span></a></li><li class=menu__item><a class=menu__link href=/tags/weight-puzzle/><span class=menu__text>Weight Puzzles</span></a></li></ul></li><li class="menu__item menu__dropdown"><a class=menu__link href=/interview-questions/><span class=menu__text>Interview Q&A</span>
<label class=drop-icon for="Interview Q&A">▾</label></a>
<input type=checkbox id="Interview Q&A"><ul class=submenu__list><li class=menu__item><a class=menu__link href=/tags/java-qa/><span class=menu__text>Java</span></a></li><li class=menu__item><a class=menu__link href=/tags/javascript-qa/><span class=menu__text>JavaScript</span></a></li><li class=menu__item><a class=menu__link href=/tags/css-qa/><span class=menu__text>CSS</span></a></li><li class=menu__item><a class=menu__link href=/tags/spring-qa/><span class=menu__text>Spring</span></a></li><li class=menu__item><a class=menu__link href=/tags/microservice-qa/><span class=menu__text>Microservices</span></a></li></ul></li><li class=menu__item><a class=menu__link href=/hugo/><span class=menu__text>Hugo</span></a></li><li class=menu__item><a class=menu__link href=/about/><span class=menu__text>About Me</span></a></li></ul></nav></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title><img class=post__thumbnail src=/img/logo/kafka.jpeg alt="Apache Kafka CCDAK Exam Notes"> Apache Kafka CCDAK Exam Notes</h1><div class="post__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>Ashish Lahoti</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2019-06-13T00:00:00Z>June 13, 2019</time>
<time class=meta__text datetime=2020-12-24T00:00:00Z>(Last Modified: December 24, 2020)</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/kafka/ rel=category>Kafka</a></span></div></div></header><div class="post__toc toc"><div class=toc__title>Page content</div><div class=toc__menu><nav id=TableOfContents><ul><li><ul><li></li></ul></li><li><a href=#frequently-asked-questions>Frequently Asked Questions</a></li><li><a href=#exam-preparation>Exam Preparation</a></li><li><a href=#sample-exam-questions>Sample Exam Questions</a><ul><li></li></ul></li><li><a href=#kafka-architecture>Kafka Architecture</a><ul><li></li></ul></li><li><a href=#kafka-cli>KAFKA CLI</a></li><li><a href=#kafka-streams>Kafka Streams</a><ul><li></li></ul></li><li><a href=#kafka-api>KAFKA API</a></li><li><a href=#confluent-schema-registry>Confluent Schema Registry</a><ul><li></li></ul></li><li><a href=#default-ports>Default Ports</a></li></ul></nav></div></div><div class="content post__content clearfix"><p>Hi Readers,</p><p>If you are planning or preparing for Apache Kafka Certification then this is the right place for you.There are many Apache Kafka Certifications are available in the market but <a href=https://www.confluent.io/certification/ target=_blank>CCDAK (Confluent Certified Developer for Apache Kafka)</a> is the most known certification as Kafka is now maintained by Confluent.</p><h6 id=ccdak-vs-ccoak>CCDAK vs CCOAK</h6><p>Confluent has introduced CCOAK certification recently. CCOAK is mainly for devOps engineer focusing on build and manage Kafka cluster. CCDAK is mainly for developers and Solution architects focusing on design, producer and consumer. If you are still not sure, I recommend to go for CCDAK as it is more comprehensive exam as compared to CCOAK. These exam notes are very helpful for both CCDAK and CCOAK certifications.</p><p>From here onward, we will talk about how to prepare for CCDAK.</p><h2 id=frequently-asked-questions>Frequently Asked Questions</h2><hr><ul><li>Prepare well for the exam as it verifies your theoretical as well as practical understanding of Kafka.</li><li>At least 40-50 hours of preparation is required.</li><li>You can register online and schedule exam on <a href=https://prod.examity.com/Confluent/ target=_blank>the Examity site</a>. I suggest to set a goal of 1 to 2 months for exam preparation and register accordingly.</li><li>Confluent kafka certification price cost at $150 for one attempt. You need to pay the fee again in order to retake exam after a gap of at least 14 days.</li><li>You need to answer 60 multiple-choice questions in 90 minutes from your laptop (with webcam) under the supervision of online proctor.</li><li>There is no negative scoring so try to answer as many questions as possible.</li><li>There is no mention of number of questions need to be correct in order to pass the exam. Result will be shown immediately (PASS or FAIL) at the end of exam. No scoring or percentage is provided.</li><li>You will receive a certificate similar to <a href=https://www.credential.net/3e05601c-df11-4ecd-8a53-3e1d21037df8 target=_blank>my CCDAK certificate</a> after passing the exam. What an achievement !!!</li><li>The certification expires after two years but you can still brag about it ;)</li><li>Don&rsquo;t waste time searching for CCDAK Certification Dumps. There ain&rsquo;t any.</li><li>Confluent has launched <a href=https://www.confluent.io/training/ target=_blank>FREE Fundamentals Accreditation Exam</a> which you can signup for free to better understand the CCDAK exam.</li></ul><h2 id=exam-preparation>Exam Preparation</h2><hr><p>I have prepared for CCDAK using following:</p><ol><li><a href=https://kafka.apache.org/documentation/ target=_blank>Apache Kafka Documentation</a></li><li><a href=https://docs.confluent.io/current/index.html target=_blank>Confluent Kafka Documentation</a></li><li><a href=https://www.confluent.io/wp-content/uploads/confluent-kafka-definitive-guide-complete.pdf target=_blank>Confluent Kafka Definitive Guide PDF</a></li><li><a href=https://www.udemy.com/course/apache-kafka/ target=_blank>Udemy Apache Kafka Series - Learn Apache Kafka for Beginners v2</a></li><li><a href=https://www.udemy.com/course/confluent-certified-developer-for-apache-kafka/ target=_blank>Udemy CCDAK 150 Practice Exam Questions</a></li></ol><p>You should prepare well for following topics. Recommended to study topics in the same sequence.</p><p>① <strong>Kafka Architecture</strong><br>Read <a href=https://www.confluent.io/wp-content/uploads/confluent-kafka-definitive-guide-complete.pdf target=_blank>Confluent Kafka Definitive Guide PDF</a> and <a href=https://kafka.apache.org/documentation/ target=_blank>Apache Kafka Documentation</a>. Once you read both, revise using <a href=#kafka-architecture>Kafka Architecture</a> section in this post.</p><p>② <strong>Kafka CLI</strong><br>Read <a href=https://www.confluent.io/wp-content/uploads/confluent-kafka-definitive-guide-complete.pdf target=_blank>Confluent Kafka Definitive Guide PDF</a> and revise using <a href=#kafka-cli>KAFKA CLI</a> section of this post.</p><p>③ <strong>Kafka Streams</strong><br>Read <a href=https://www.confluent.io/wp-content/uploads/confluent-kafka-definitive-guide-complete.pdf target=_blank>Confluent Kafka Definitive Guide PDF</a> and revise using <a href=#kafka-streams>Kafka Streams</a> section of this post.</p><p>④ <strong>Kafka Security</strong><br>Read <a href=https://kafka.apache.org/documentation/#security target=_blank>Apache Kafka Documentation Security Section</a></p><p>⑤ <strong>Kafka APIs</strong><br>Read <a href=https://kafka.apache.org/documentation/#api target=_blank>Apache Kafka Documentation API section</a> and revise using <a href=#kafka-api>KAFKA API</a> section of this post.</p><p>⑥ <strong>Kafka Monitoring (Metrics)</strong><br>Read <a href=https://www.confluent.io/wp-content/uploads/confluent-kafka-definitive-guide-complete.pdf target=_blank>Confluent Kafka Definitive Guide PDF</a> and <a href=https://kafka.apache.org/documentation/ target=_blank>Apache Kafka Documentation</a> for important metrics. Read <a href=https://docs.confluent.io/current/index.html target=_blank>Confluent Kafka Documentation</a> as well.</p><p>⑦ <strong>Confluent Schema Registry</strong><br>Read <a href=https://docs.confluent.io/current/index.html target=_blank>Confluent Kafka Documentation</a> and revise using <a href=#confluent-schema-registry>Confluent Schema Registry</a> section of this post.</p><p>⑧ <strong>Confluent KSQL</strong><br>Read <a href=https://docs.confluent.io/current/streams-ksql.html target=_blank>Confluent Kafka Documentation KSQL and Kafka Streams section</a></p><p>⑨ <strong>Confluent REST Proxy</strong><br>Read <a href=https://docs.confluent.io/current/kafka-rest/index.html# target=_blank>Confluent Kafka Documentation Rest Proxy Section</a></p><h2 id=sample-exam-questions>Sample Exam Questions</h2><hr><p>Please note that these are not the actual questions from the CCDAK exam but most likelihood to ask in exam.</p><h5 id=1-kafka-theory>1. Kafka Theory</h5><ul><li>Kafka is a &mldr;. ?<br><em><strong>pub-sub system</strong></em></li><li>Mostly Kafka is written in which language?<br><em><strong>Scala</strong></em></li><li>Which errors are retriable from Kafka Producer?<br><em><strong>LEADER_NOT_AVAILABLE, NOT_LEADER_FOR_PARTITION, UNKNOWN_TOPIC_OR_PARTITION</strong></em></li><li>What is a generic unique id which can be used for a messages received from a consumer?<br><em><strong>Topic + Partition + Offset</strong></em></li></ul><blockquote><p>Read <a href=#kafka-architecture>Kafka Architecture</a> section of this post for more questions and answers</p></blockquote><h5 id=2-kafka-streams>2. Kafka Streams</h5><ul><li>To transform data from a Kafka topic to another one, You should use?<br><em><strong>Kafka Streams</strong></em></li><li>Which of the Kafka Stream operators are stateful?</li><li>Which of the Kafka Stream operators are stateless?</li><li>Which window is not having gap?</li><li>Which Kafka Stream joins doesn&rsquo;t require co-partition of data?<br><em><strong>KStream-to-GlobalKTable</strong></em></li><li>Which Kafka Stream joins is always windowed join?<br><em><strong>KStream-to-KStream</strong></em></li><li>What is the output of KStream-to-KTable join?<br><em><strong>KStream</strong></em></li></ul><blockquote><p>Read <a href=#kafka-streams>Kafka Streams</a> section of this post for answers</p></blockquote><h5 id=3-confluent-schema-registry>3. Confluent Schema Registry</h5><ul><li>Which of the following is not a primitive type of Avro?</li><li>Which of the following in not a complex type of Avro?</li><li>Which of the following is not a required field in Avro Schema?</li><li>Delete a field without default value in Avro schema is &mldr;&mldr; compatibility?<br><em><strong>backward</strong></em></li><li>Adding a field to record without default value is &mldr;&mldr; schema evolution?<br><em><strong>forward</strong></em></li><li>In Avro, removing or adding a field that has a default value is a &mldr;&mldr; schema evolution?<br><em><strong>full</strong></em></li><li>What client protocols are supported for the schema registry?<br><em><strong>HTTP, HTTPS</strong></em></li><li>Where are Avro schema stored in Confluent Schema Registry?<br><em><strong>_schemas Kafka topic</strong></em></li></ul><blockquote><p>Read <a href=#confluent-schema-registry>Confluent Schema Registry</a> section of this post for answers</p></blockquote><h5 id=4-confluent-ksql>4. Confluent KSQL</h5><ul><li>is KSQL ANSI SQL Compliant?<br><em><strong>No</strong></em></li><li>What Java library is KSQL based on?<br><em><strong>Kafka Streams</strong></em></li></ul><h5 id=5-kafka-security>5. Kafka Security</h5><ul><li>What are the valid authentication mechanism in KAFKA?<br><em><strong>SSL</strong></em><br><em><strong>SASL/GSSAPI (Kerberos)</strong></em><br><em><strong>SASL/PLAIN</strong></em><br><em><strong>SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512</strong></em><br><em><strong>SASL/OAUTHBEARER</strong></em></li></ul><h2 id=kafka-architecture>Kafka Architecture</h2><hr><p>☛ I have spent so much time preparing these notes. You can expect most of the questions related to Kafka architecture from these notes.
☛ Moreover, If you are preparing for interviews then you can also expect most of the Kafka interview questions from these notes.</p><h4 id=cluster>Cluster</h4><ul><li>Cluster is group of Kafka brokers.</li></ul><h4 id=rack>Rack</h4><ul><li>A racks belongs to a cluster.</li><li>A broker belongs to a rack when property <strong>broker.rack=&lt;rack-id></strong> is specified at broker level. This enables the rack awareness feature which spreads replicas of the same partition across different racks.</li><li>Let&rsquo;s say you have 6 brokers and 2 racks. Brokers 1, 2, 3 are on the <strong>rack_1</strong>, and brokers 4, 5, 6 are on <strong>rack_2</strong>.<br>Now when you create a topic with 6 partition, instead of assigning broker to partition in order from 1, 2, 3, 4, 5, 6, each partition is assigned to each rack repeatedly i.e. 1, 4, 2, 5, 3, 6.</li></ul><h4 id=broker>Broker</h4><ul><li>Every broker in Kafka is a <strong>bootstrap server</strong> which knows about all brokers, topics and partitions (metadata) that means Kafka client (e.g. producer,consumer etc) only need to connect to one broker in order to connect to entire cluster.</li><li>At all times, only one broker should be the controller, and one broker must always be the controller in the cluster</li></ul><h4 id=topic>Topic</h4><ul><li>Kafka takes bytes as input without even loading them into memory (that&rsquo;s called <strong>zero copy</strong>)</li><li>Brokers have defaults for all the topic configuration parameters</li></ul><h4 id=partition>Partition</h4><ul><li>Topic can have one or more partition.</li><li>Each partition can have one or more replica decided by <em>Replication Factor</em>. Replication Factor = 3 means One leader partition and two replicas.</li><li>It is not possible to delete a partition of topic once created.</li><li>Order is guaranteed within the partition and once data is written into partition, <strong>its immutable!</strong></li><li>If producer writes at 1 GB/sec and consumer consumes at 250MB/sec then requires 4 partition!</li></ul><h4 id=segment>Segment</h4><ul><li>Partitions are made of segments <em>(<strong>*.log</strong> files)</em></li><li>At a time only one segment is active in a partition</li><li>Segment stores the actual Kafka messages along with offset, timestamp, compression etc.</li><li><strong>log.segment.bytes = 1 GB (default)</strong>, Max size in bytes to close the segment and roll over to new segment.</li><li><strong>log.segment.ms = 1 Week (default)</strong>, Max time in ms to wait before closing the segment even if it is not full or reached Max Size.</li><li>Every segment also has two indexes (files):-<ol><li>An offset to position index <em>(<strong>*.index</strong> file)</em> - Allows kafka where to read to find a message</li><li>A timestamp to offset index <em>(<strong>*.timeindex</strong> file)</em> - Allows kafka to find a message with a timestamp</li></ol></li></ul><h4 id=log-retention-and-cleanup-policies>Log Retention and Cleanup policies</h4><ul><li>Log cleanup happen on partition segments (.log files). Smaller or more segments means the log cleanup will happen more often!</li><li>Old segments are cleaned up based on time-based and size-based log retention policies whichever happens first.</li><li><strong>Time Based Retention:</strong> The segment older by configured retention time eligible for Cleanup. You can use any of the three configuration <strong>log.retention.ms</strong>, <strong>log.retention.minutes</strong>, or <strong>log.retention.hours</strong> to specify retention period in ms, minutes or hours. If ms is not set, minutes is used. If minutes is not set, hours is used. <strong>Default</strong> retention time is <strong>1 Week</strong>.</li><li><strong>Size Based Retention:</strong> The older segment are cleaned up when the max configured size of a topic partition (includes all segments), is reached. You can use <strong>log.retention.bytes=-1 (default is infinite)</strong> to configure max size in bytes.</li><li>Cleanup process checks any logs to cleanup in every <strong>log.cleaner.backoff.ms=15 seconds (default)</strong>.</li><li><strong>Cleanup Policy:</strong> Logs are deleted, compacted, or both based on <strong>log.cleanup.policy</strong>. Delete policy discard the old segments when their retention time or size is reached. Compact policy delete the old messages per key and keep the latest copy for that key.</li><li>Deleted records can still be seen by consumers for a period of <strong>delete.retention.ms=24 hours (default)</strong></li></ul><h4 id=offset>Offset</h4><ul><li>Each Topic Partition is having its own offset starting from 0.</li></ul><h4 id=topic-replication>Topic Replication</h4><ul><li>Replication factor = 3 and partition = 2 means there will be total 6 partition distributed across Kafka cluster. Each partition will be having 1 leader and 2 ISR <em>(in-sync replica)</em>.</li><li>Broker contains leader partition called <strong>leader</strong> of that partition and only leader can receive and serve data for partition.</li><li>Replication factor can not be greater then number of broker in the kafka cluster. If topic is having a replication factor of 3 then each partition will live on 3 different brokers.</li></ul><h4 id=producer>Producer</h4><ul><li>Kafka Producer automatically recover from following <strong>retriable errors</strong>:<br><em>LEADER_NOT_AVAILABLE</em><br><em>NOT_LEADER_FOR_PARTITION</em></li><li>Kafka Producer throw error for following <strong>non-retriable errors</strong>:<br><em>OFFSET_OUT_OF_RANGE</em><br><em>BROKER_NOT_AVAILABLE</em><br><em>MESSAGE_TOO_LARGE</em><br><em>INVALID_TOPIC_EXCEPTION</em></li><li>If you send a message of size 3 MB to a topic with default message size configuration. Then producer will throw MessageSizeTooLarge exception immediately since it is not a retriable exception.</li><li>When produce to a topic which doesn&rsquo;t exist and <strong>auto.create.topic.enable=true</strong> then kafka creates the topic automatically with the broker/topic settings <strong>num.partition</strong> and <strong>default.replication.factor</strong></li></ul><h4 id=producer-acknowledgment>Producer Acknowledgment</h4><ul><li><p><strong>acks=0</strong> Producer do not wait for ack <em>( possible data loss )</em></p></li><li><p><strong>acks=1</strong> Producer wait for leader ack <em>( limited data loss )</em></p></li><li><p><strong>acks=all</strong> Producer wait for leader+replica ack <em>( no data loss )</em></p></li><li><p><strong>acks=all</strong> must be used in conjunction with <strong>min.insync.replicas</strong> which can be set at broker or topic level.</p><p>*<em>( assuming that replicas are distributed across 3 brokers for below points )</em></p><ul><li><strong>min.insync.replica</strong> only matters if acks=all</li><li><strong>acks=all, min.insync.replica=2</strong> implies that at least 2 brokers that are ISR <em>(including leader)</em> must acknowledge</li><li><strong>acks=all, min.insync.replica=1</strong> implies that at least 1 brokers that is ISR <em>(including leader)</em> must acknowledge</li><li>A kafka topic with <strong>replication.factor=3, acks=all, min.insync.replicas=2</strong> can only tolerate 1 broker going down, otherwise the producer will receive an exception NOT_ENOUGH_REPLICAS on send.</li><li>A kafka topic with <strong>replication.factor=3, acks=all, min.insync.replicas=1</strong>, can tolerate maximum number of 2 brokers going down, so that a producer can still produce to the topic.</li></ul></li></ul><h4 id=producer-configuration>Producer Configuration</h4><ul><li>Mandatory properties to configure Kafka producer is as follows:<br><em>bootstrap.servers</em><br><em>key.serializer</em><br><em>value.serializer</em></li></ul><h4 id=safe-producer-configuration>Safe Producer Configuration</h4><ul><li><strong>min.insync.replicas=2</strong> (set at broker or topic level)</li><li><strong>retries=MAX_INT</strong> number of reties by producer in case of transient failure/exception. (default is 0)</li><li><strong>max.in.flight.per.connection number=5</strong> number of producer request can be made in parallel (default is 5)</li><li><strong>acks=all</strong></li><li><strong>enable.idempotence=true</strong> producer send producerId with each message to identify for duplicate msg at kafka end. When kafka receives duplicate message with same producerId which it already committed. It do not commit it again and send ack to producer (default is false)</li></ul><h4 id=high-throughput-producer-using-compression-and-batching>High Throughput Producer using compression and batching</h4><ul><li><strong>compression.type=snappy</strong> value can be none(default), gzip, lz4, snappy. Compression is enabled at the producer level and doesn&rsquo;t require any config change in broker or consumer Compression is more effective in case of bigger batch of messages being sent to kafka</li><li><strong>linger.ms=20</strong> Number of millisecond a producer is willing to wait before sending a batch out. (default 0). Increase linger.ms value increase the chance of batching.</li><li><strong>batch.size=32KB or 64KB</strong> Maximum number of bytes that will be included in a batch (default 16KB). Any message bigger than the batch size will not be batched</li></ul><h4 id=message-key>Message Key</h4><ul><li>Producer can choose to send a key with message.</li><li>If key = null, data is send in round robin</li><li>If key is sent, then all message for that key will always go to same partition. This can be used to order the messages for a specific key since order is guaranteed in same partition.</li><li>Adding a partition to the topic will loose the guarantee of same key go to same partition.</li><li>Keys are hashed using <strong>murmur2</strong> algorithm by default.</li></ul><h4 id=message-size>Message Size</h4><ul><li>Default Message max size is 1MB</li><li>If you try to send message > 1MB then <em>MessageSizeTooLargeException</em> is thrown</li><li>Suppose you want to send a 15MB of message from producer to broker to consumer successfully then you need to configure:-<ol><li><strong>message.max.bytes=15728640</strong> (broker/topic config) - is max size of message that can be received by the broker from producer.</li><li><strong>replica.fetch.max.bytes=15728640</strong> (broker config) - is max size of message that can be replicated across brokers.</li><li><strong>fetch.message.max.bytes=15728640</strong> (consumer config) - is max size of message that can be fetched by consumer.</li></ol></li></ul><h4 id=consumer>Consumer</h4><ul><li>Per thread one consumer is the rule. Consumer must not be multi threaded.</li><li>Each consumer is assigned to different partition in same consumer group.<ul><li>If there are 5 consumers of same consumer group consuming from a topic with 10 partition then 2 partitions will be assigned to each consumer and no consumer will remain idle.</li><li>If there are 10 consumers of same consumer group consuming from a topic with 5 partition then 5 partition will be assigned to 5 consumers and rest 5 consumers will remain idle.</li></ul></li><li><strong>records-lag-max</strong> (monitoring metrics) The maximum lag in terms of number of records for any partition in this window. An increasing value over time is your best indication that the consumer group is not keeping up with the producers.</li></ul><h4 id=consumer-group>Consumer Group</h4><ul><li>If two applications want to process all the messages independently from a kafka topic having 4 partition, then you should create 2 consumer groups with 4 consumers in each group for optimal performance.</li></ul><h4 id=consumer-offset>Consumer Offset</h4><p>When consumer in a group has processed the data received from Kafka, it commits the offset in Kafka topic named <strong>_consumer_offset</strong> which is used when a consumer dies, it will be able to read back from where it left off.</p><h4 id=delivery-semantics>Delivery Semantics</h4><ul><li><strong>At most once</strong> Offset are committed as soon as message batch is received. If the processing goes wrong, the message will be lost (it won&rsquo;t be read again)</li><li><strong>At least once (default)</strong> Offset are committed after the message is processed. If the processing goes wrong, the message will be read again. This can result in duplicate processing of message.<ul><li>Make sure your processing is idempotent. (i.e. re-processing the message won&rsquo;t impact your systems). For most of the application, we use this and ensure processing are idempotent.</li></ul></li><li><strong>Exactly once</strong> Can only be achieved for Kafka=>Kafka workflows using Kafka Streams API. For Kafka=>Sink workflows, use an idempotent consumer.</li></ul><h4 id=consumer-offset-commit-strategy>Consumer Offset commit strategy</h4><ul><li><strong>enable.auto.commit=true & synchronous processing of batches</strong> with auto commit, offset will be committed automatically for you at regular interval (auto.commit.interval.ms=5000 by default) every time you call .poll(). If you don&rsquo;t use synchronous processing, you will be in &ldquo;at most once&rdquo; behavior because offsets will be committed before your data is processed.</li><li><strong>enable.auto.commit=false & manual commit of offsets (recommended)</strong></li></ul><h4 id=consumer-offset-reset-behavior>Consumer Offset reset behavior</h4><ul><li><strong>auto.offset.reset=latest</strong> will read from the end of the log</li><li><strong>auto.offset.reset=earliest</strong> will read from the start of the log</li><li><strong>auto.offset.reset=none</strong> will throw exception of no offset is found</li><li>Consumer offset can be lost if hasn&rsquo;t read new data in 7 days. This can be controlled by broker setting offset.retention.minutes</li></ul><h4 id=consumer-poll-behavior>Consumer Poll Behavior</h4><ul><li><strong>fetch.min.bytes = 1 (default)</strong>, Control how much data you want to pull at least on each request. Help improving throughput and decreasing request number. At the cost of latency.</li><li><strong>max.poll.records = 500 (default)</strong>, Controls how many records to receive per poll request. Increase if your messages are very small and have a lot of available RAM.</li><li><strong>max.partition.fetch.bytes = 1MB (default)</strong>, Maximum data returned by broker per partition. If you read from 100 partition, you will need a lot of memory (RAM)</li><li><strong>fetch.max.bytes = 50MB (default)</strong>, Maximum data returned for each fetch request (covers multiple partition). Consumer performs multiple fetches in parallel.</li></ul><h4 id=consumer-heartbeat-thread>Consumer Heartbeat Thread</h4><ul><li>Heartbeat mechanism is used to detect if consumer application in dead.</li><li><strong>session.timeout.ms=10s (default)</strong>, If heartbeat is not sent in 10 second period, the consumer is considered dead. Set lower value to faster consumer rebalances</li><li><strong>heartbeat.interval.ms=3s (default)</strong>, Heartbeat is sent in every 3 seconds interval. Usually 1/3rd of session.timeout.ms</li></ul><h4 id=consumer-poll-thread>Consumer Poll Thread</h4><ul><li>Poll mechanism is also used to detect if consumer application is dead.</li><li><strong>max.poll.interval.ms = 5min (default)</strong>, Max amount of time between two .poll() calls before declaring consumer dead. If processing of message batch takes more time in general in application then should increase the interval.</li></ul><h4 id=kafka-guarantees>Kafka Guarantees</h4><ul><li>Messages are appended to a topic-partition in the order they are sent</li><li>Consumer read the messages in the order stored in topic-partition</li><li>With a replication factor of N, producers and consumers can tolerate upto N-1 brokers being down</li><li>As long as number of partitions remains constant for a topic ( no new partition), the same key will always go to same partition</li></ul><h4 id=client-bi-directional-compatibility>Client Bi-Directional Compatibility</h4><ul><li>an Older client (1.1) can talk to Newer broker (2.0)</li><li>a Newer client (2.0) can talk to Older broker (1.1)</li></ul><h4 id=kafka-connect>Kafka Connect</h4><ul><li><strong>Source connect</strong> Get data from common data source to Kafka for e.g. import data from external database to kafka</li><li><strong>Sink connect</strong> Publish data from Kafka to common data source for e.g. export data from Kafka to external database</li></ul><h4 id=zookeeper>Zookeeper</h4><ul><li><p>ZooKeeper servers will be deployed on multiple nodes. This is called an <strong>ensemble</strong>. An ensemble is a set of <strong>2n + 1</strong> ZooKeeper servers where n is any number greater than 0. The odd number of servers allows ZooKeeper to perform majority elections for leadership. At any given time, there can be up to n failed servers in an ensemble and the ZooKeeper cluster will keep quorum. If at any time, quorum is lost, the ZooKeeper cluster will go down.</p></li><li><p>Zookeeper cluster to withstand loss of 2 server, require total of 2*2+1 = 5 servers.</p></li><li><p>In Zookeeper multi-node configuration, <strong>initLimit</strong> and <strong>syncLimit</strong> are used to govern how long following ZooKeeper servers can take to initialize with the current leader and how long they can be out of sync with the leader.
If tickTime=2000, initLimit=5 and syncLimit=2 then a follower can take (tickTime<em>initLimit) = 10000ms to initialize and may be out of sync for up to (tickTime</em>syncLimit) = 4000ms</p></li><li><p>In Zookeeper multi-node configuration, The server.* properties set the ensemble membership. The format is</p><p><b>server.&lt;myid>=&lt;hostname>:&lt;leaderport>:&lt;electionport></b>, where:</p><ul><li><strong>myid</strong> is the server identification number. In this example, there are three servers, so each one will have a different myid with values 1, 2, and 3 respectively. The myid is set by creating a file named myid in the dataDir that contains a single integer in human readable ASCII text. This value must match one of the myid values from the configuration file. If another ensemble member has already been started with a conflicting myid value, an error will be thrown upon startup.</li><li><strong>leaderport</strong> is used by followers to connect to the active leader. This port should be open between all ZooKeeper ensemble members.</li><li><strong>electionport</strong> is used to perform leader elections between ensemble members. This port should be open between all ZooKeeper ensemble members.</li></ul></li></ul><h2 id=kafka-cli>KAFKA CLI</h2><hr><p>① Start a zookeeper at default port 2181</p><pre><code>$bin/zookeeper-server-start.sh config/zookeeper.properties
</code></pre><p>② Start a kafka server at default port 9092</p><pre><code>$bin/kafka-server-start.sh config/server.properties
</code></pre><p>③ Create a kafka topic &lsquo;my-first-topic&rsquo; with 3 partitions and 3 replicas</p><pre><code>$bin/kafka-topics.sh --zookeeper localhost:2181 --topic my-first-topic --create --replication-factor 3 --partitions 3
</code></pre><p>④ List all kafka topics</p><pre><code>$bin/kafka-topics.sh --zookeeper localhost:2181 --list
</code></pre><p>⑤ Describe kafka topic &lsquo;my-first-topic&rsquo;</p><pre><code>$bin/kafka-topics.sh --zookeeper localhost:2181 --topic my-first-topic --describe
</code></pre><p>⑥ Delete kafka topic &lsquo;my-first-topic&rsquo;</p><pre><code>$bin/kafka-topics.sh --zookeeper localhost:2181 --topic my-first-topic --delete
</code></pre><p><strong>Note:</strong> This will have no impact if delete.topic.enable is not set to true</p><p>⑦ Find out all the partitions without a leader</p><pre><code>$bin/kafka-topics.sh --zookeeper localhost:2181 --describe --unavailable-partitions
</code></pre><p>⑧ Produce messages to Kafka topic my-first-topic</p><pre><code>$bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-first-topic --producer-property acks=all 
&gt; message 1  
&gt; message 2  
&gt; ^C
</code></pre><p>⑨ Start Consuming messages from kafka topic my-first-topic</p><pre><code>$bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic my-first-topic --from-beginning
&gt; message 1  
&gt; message 2
</code></pre><p>⑩ Start Consuming messages in a consumer group from kafka topic my-first-topic</p><pre><code>$bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic my-first-topic --group my-first-consumer-group --from-beginning
</code></pre><p>⑪ List all consumer groups</p><pre><code>$bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list
</code></pre><p>⑫ Describe consumer group</p><pre><code>$bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe -group my-first-consumer-group
</code></pre><p>⑬ Reset offset of consumer group to replay all messages</p><pre><code>$bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe -group my-first-consumer-group --reset-offsets --to-earliest --execute --topic my-first-topic
</code></pre><p>⑭ Shift offsets by 2 (forward) as another strategy</p><pre><code>bin/kafka-consumer-groups --bootstrap-server localhost:9092 --group my-first-consumer-group --reset-offsets --shift-by 2 --execute --topic my-first_topic
</code></pre><p>⑮ Shift offsets by 2 (backward) as another strategy</p><pre><code>bin/kafka-consumer-groups --bootstrap-server localhost:9092 --group my-first-consumer-group --reset-offsets --shift-by -2 --execute --topic my-first_topic
</code></pre><h2 id=kafka-streams>Kafka Streams</h2><hr><p>Kafka Streams is used for building streaming applications which transform data of one Kafka topics and feeds to another Kafka topic.</p><h4 id=1-stateless-operators>1. Stateless Operators</h4><pre><code>branch
filter
inverseFilter
flatMap
flatMapValues
foreach
groupByKey
groupBy
map
mapValues
</code></pre><h4 id=2-stateful-operators>2. Stateful Operators</h4><pre><code>join
aggregate
count
reduce
windowing
</code></pre><h4 id=3-window>3. Window</h4><h6 id=1-tumbling-window>1) Tumbling window</h6><ul><li>Time based, Fixed Size, Non overlapping, Gap less windows</li><li>For e.g. if window-size=5min and advance-interval =5min then it looks like [0-5min] [5min-10min] [10min-15min]&mldr;..</li></ul><h6 id=2-hopping-window>2) Hopping window</h6><ul><li>Time based, Fixed Size, Overlapping windows</li><li>For e.g. if widow-size=5min and advance-interval=3min then it looks like [0-5min] [3min-8min] [6min-11min]&mldr;&mldr;</li></ul><h6 id=3-sliding-window>3) Sliding window</h6><ul><li>Fixed size overlapping window that works on the difference between record timestamp</li><li>Used only for join operation</li></ul><h6 id=4-session-window>4) Session window</h6><ul><li>Session based, Dynamically sized, Non overlapping, Data driven window.</li><li>Used to aggregate key based events into session.</li></ul><p>For more information, refer <a href=https://kafka.apache.org/20/documentation/streams/developer-guide/dsl-api.html#windowing target=_blank>Apache Kafka Documentation on windowing</a></p><h4 id=4-serdes-data-types>4. SerDes data types</h4><p>Kafka stream operations require SerDes (Serializer/Deserializer) to identify data type.</p><pre><code>byte[]
ByteBuffer
Double
Integer
Long
String
</code></pre><h4 id=5-streams-dsl>5. Streams DSL</h4><h6 id=1-kstream>1) KStream</h6><ul><li>Abstraction of <strong>record stream</strong> from subset of partitions of topic</li><li>In database table analogy, interpreted as <em><strong>INSERT</strong></em> statement</li><li>In an e-commerce application, any type of transactions like purchase, payment should be modeled as KStream</li></ul><h6 id=2-ktable>2) KTable</h6><ul><li>Abstraction of <strong>changelog stream</strong> from subset of partitions of topic</li><li>In database table analogy, interpreted as <em><strong>UPDATE</strong></em> statement</li><li>In an e-commerce application, mostly static data like inventory list, customer list and aggregated data like total sales should be modeled as KTable</li></ul><h6 id=3-globalktable>3) GlobalKTable</h6><ul><li>Abstraction of <strong>changelog stream</strong> from all partitions of topic</li><li>In database table analogy, interpreted as <em><strong>UPDATE</strong></em> statement</li></ul><p>For more information, refer <a href=https://kafka.apache.org/25/documentation/streams/developer-guide/dsl-api.html target=_blank>Apache Kafka Documentation on stream DSL</a></p><h4 id=6-join-operands>6. Join Operands</h4><div class=table-scroll><table><thead><tr><th align=left>Join Operands</th><th align=left>Output</th><th align=left>Type</th><th align=left>co-partition required</th><th align=left>Join Type</th></tr></thead><tbody><tr><td align=left>KStream-to-KStream</td><td align=left>KStream</td><td align=left>Windowed</td><td align=left>Yes</td><td align=left>key and window based</td></tr><tr><td align=left>KTable-to-KTable</td><td align=left>KTable</td><td align=left>Non-windowed</td><td align=left>Yes</td><td align=left>key or foreign-key based</td></tr><tr><td align=left>KStream-to-KTable</td><td align=left>KStream</td><td align=left>Non-windowed</td><td align=left>Yes</td><td align=left>key based</td></tr><tr><td align=left>KStream-to-GlobalKTable</td><td align=left>KStream</td><td align=left>Non-windowed</td><td align=left>No</td><td align=left>key or foreign-key based</td></tr></tbody></table></div><h6 id=co-partition>co-partition</h6><p>co-partition means both the left and right join operand topics must have same number of partitions.</p><p>A join between a topic A ( 5 parition ) and topic B (3 partition) is possible using <em>KStream-to-GlobalKTrade</em> since it does not require co-partition.</p><h2 id=kafka-api>KAFKA API</h2><hr><ul><li><a href=https://codingnconcepts.com/post/apache-kafka-producer-using-java/ target=_blank>Click here</a> to find out how we can create a Safe and high throughput Kafka Producer using Java.</li><li><a href=https://codingnconcepts.com/post/apache-kafka-consumer-using-java/ target=_blank>Click here</a> to find out how we can create a Kafka consumer using Java with manual auto commit enabled.</li></ul><h2 id=confluent-schema-registry>Confluent Schema Registry</h2><hr><h4 id=1-avro>1. Avro</h4><h6 id=primitive-types>Primitive Types</h6><pre><code>1. null
2. boolean
3. int (32 bit)
4. long (64 bit)
5. float (32 bit)
6. double (64 bit)
7. byte[] (8 bit)
8. string (char sequence)
</code></pre><h6 id=complex-types>Complex Types</h6><pre><code>1. record
2. enum
3. array
4. map
5. union
6. fixed
</code></pre><h6 id=avro-schema-definition>Avro Schema Definition</h6><pre><code>namespace (required)
type (required) =&gt; record, enum, array, map, union, fixed
name (required)
doc (optional)
aliases (optional)
fields (required) {
    name (required)
    type (required)
    doc (optional)
    default (optional)
    order (optional)
    aliases (optional)
}
</code></pre><p><br></p><h4 id=2-confluent-schema-notes>2. Confluent Schema Notes</h4><ul><li>Schema Registry stores all schemas in a Kafka topic _schemas defined by <strong>kafkastore.config = _schemas (default)</strong> which is a single partition topic with log compacted.</li><li>The default response media type <strong>application/vnd.schemaregistry.v1+json, application/vnd.schemaregistry+json, application/json</strong> are used in response header.</li><li><strong>HTTP</strong> and <strong>HTTPS</strong> client protocol are supported for schema registry.</li><li>Prefix to apply to metric names for the default JMX reporter <strong>kafka.schema.registry</strong></li><li>Default port for listener is <strong>8081</strong></li><li>Confluent support primitive types of <strong>null, Boolean, Integer, Long, Float, Double, String, byte[]</strong>, and complex type of <strong>IndexedRecord</strong>. Sending data of other types to KafkaAvroSerializer will cause a SerializationException</li></ul><p><br></p><h4 id=3-confluent-schema-compatibility-types>3. Confluent Schema Compatibility Types</h4><h6 id=backward>BACKWARD</h6><ul><li>Consumer using schema X can process data produced with schema X or X-1. In case of BACKWARD_TRANSITIVE, consumer using schema X can process data produced with all previous schema X, X-1, X-2 and so on</li><li>Delete field without default value (Required field) is allowed. In this case, Consumer ignore this field.</li><li>Add field with default value (Optional field) is allowed. In this case, Consumer will assign default value.</li><li>BACKWARD is default compatibility type in confluent schema registry.</li><li>There is no assurance that consumers using older schema can read data produced using the new schema. Therefore, upgrade all consumers before you start producing new events.</li></ul><h6 id=forward>FORWARD</h6><ul><li>Data produced using schema X can be ready by consumers with schema X or X-1. In case of FORWARD_TRANSITIVE, data produced using schema X can be ready by consumers with all previous schema X, X-1, X-2 and so on</li><li>Add field without default value (Required field) is allowed. In this case, Consumer ignore this field.</li><li>Delete field with default value (Optional field) is allowed. In this case, Consumer will assign default value.</li><li>There is no assurance that consumers using the new schema can read data produced using older schema. Therefore, first upgrade all producers to using the new schema and make sure the data already produced using the older schema are not available to consumers, then upgrade the consumers.</li></ul><h6 id=full>FULL</h6><ul><li>Backward and forward compatible between schema X and X-1. In case of FULL_TRANSITIVE, backward and forward compatible between all previous schema X and X-1 and X-2 and so on</li><li>Modify field with default value (Optional field) is allowed.</li><li>There are assurances that consumers using older schema can read data produced using the new schema and that consumers using the new schema can read data produced using older schema. Therefore, you can upgrade the producers and consumers independently.</li></ul><h6 id=none>NONE</h6><ul><li>Compatibility type means schema compatibility checks are disabled.</li><li>Upgrading Consumer or Producer depends. For example, modifying a field type from Number to String. In this case, you will either need to upgrade all producers and consumers to the new schema version at the same time</li></ul><h2 id=default-ports>Default Ports</h2><hr><ul><li>Zookeeper Client Port: 2181</li><li>Zookeeper Leader Port: 3888</li><li>Zookeeper Election Port (Peer port): 2888</li><li>Broker: 9092</li><li>REST Proxy: 8082</li><li>Schema Registry: 8081</li><li>KSQL: 8088</li></ul></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5.0 11V3C0 1.5.8.8.8.8S1.5.0 3 0h8c1.5.0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 100-6 3 3 0 000 6z"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=/tags/confluent-kafka-ccdak/ rel=tag>Confluent Kafka CCDAK</a></li><li class=tags__item><a class="tags__link btn" href=/tags/popular-posts/ rel=tag>Popular Posts</a></li></ul></div></footer></article></main><section class=social-share><ul class=share-icons><li><a href="https://twitter.com/intent/tweet?hashtags=codingnconcepts&url=https%3a%2f%2fcodingnconcepts.com%2fpost%2fapache-kafka-ccdak-exam-notes%2f&text=Apache%20Kafka%20CCDAK%20Exam%20Notes" target=_blank rel=noopener aria-label="Share on Twitter" class="share-btn twitter"><svg class="widget-social__link-icon icon icon-twitter" width="24" height="24" viewBox="0 0 384 312"><path d="m384 36.9c-14.1 6.3-29.3 10.5-45.2 12.4 16.3-9.7 28.8-25.2 34.6-43.6-15.2 9-32.1 15.6-50 19.1-14.4-15.2-34.9-24.8-57.5-24.8-43.5.0-78.8 35.3-78.8 78.8.0 6.2.7 12.2 2 17.9-65.5-3.3-123.5-34.6-162.4-82.3-6.7 11.6-10.6 25.2-10.6 39.6.0 27.3 13.9 51.4 35 65.6-12.9-.4-25.1-4-35.7-9.9v1c0 38.2 27.2 70 63.2 77.2-6.6 1.8-13.6 2.8-20.8 2.8-5.1.0-10-.5-14.8-1.4 10 31.3 39.1 54.1 73.6 54.7-27 21.1-60.9 33.7-97.8 33.7-6.4.0-12.6-.4-18.8-1.1 34.9 22.4 76.3 35.4 120.8 35.4 144.9.0 224.1-120 224.1-224.1.0-3.4-.1-6.8-.2-10.2 15.4-11.1 28.7-25 39.3-40.8z"/></svg><p>Twitter</p></a></li><li><a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fcodingnconcepts.com%2fpost%2fapache-kafka-ccdak-exam-notes%2f" target=_blank rel=noopener aria-label="Share on Facebook" class="share-btn facebook"><svg class="widget-social__link-icon icon icon-facebook" width="24" height="24" viewBox="0 0 352 352"><path d="m0 32v288c0 17.5 14.5 32 32 32h288c17.5.0 32-14.5 32-32V32c0-17.5-14.5-32-32-32H32C14.5.0.0 14.5.0 32zm320 0v288h-83V212h41.5l6-48H237v-31c0-14 3.5-23.5 23.5-23.5h26V66c-4.4-.6-19.8-1.5-37.5-1.5-36.9.0-62 22.2-62 63.5v36h-42v48h42v108H32V32z"/></svg><p>Facebook</p></a></li><li><a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fcodingnconcepts.com%2fpost%2fapache-kafka-ccdak-exam-notes%2f&source=https%3a%2f%2fcodingnconcepts.com%2fpost%2fapache-kafka-ccdak-exam-notes%2f&title=Apache%20Kafka%20CCDAK%20Exam%20Notes&summary=Apache%20Kafka%20CCDAK%20Exam%20Notes" target=_blank rel=noopener aria-label="Share on LinkedIn" class="share-btn linkedin"><svg class="widget-social__link-icon icon icon-linkedin" width="24" height="24" viewBox="0 0 352 352"><path d="M0 40v272c0 21.9 18.1 40 40 40h272c21.9.0 40-18.1 40-40V40c0-21.9-18.1-40-40-40H40C18.1.0.0 18.1.0 40zm312-8c4.6.0 8 3.4 8 8v272c0 4.6-3.4 8-8 8H40c-4.6.0-8-3.4-8-8V40c0-4.6 3.4-8 8-8H312zM59.5 87c0 15.2 12.3 27.5 27.5 27.5s27.5-12.3 27.5-27.5S102.2 59.5 87 59.5 59.5 71.8 59.5 87zM187 157h-1v-21h-45v152h47v-75c0-19.8 3.9-39 28.5-39 24.2.0 24.5 22.4 24.5 40v74h47v-83.5c0-40.9-8.7-72-56.5-72C208.5 132.5 193.3 145.1 187 157zM64 288h47.5V136H64V288z"/></svg><p>LinkedIn</p></a></li><li><a href="whatsapp://send?text=Apache%20Kafka%20CCDAK%20Exam%20Notes%2c%20by%20Coding%20N%20Concepts%0aStudy%20guide%20for%20Kafka%20Certification%20CCDAK%20%28Certified%20Developer%20for%20Apache%20Kafka%29%20and%20CCOAK%20%28Certified%20Operator%20for%20Apache%20Kafka%29%20preparation%20with%20sample%20exam%20questions.%0a%0ahttps%3a%2f%2fcodingnconcepts.com%2fpost%2fapache-kafka-ccdak-exam-notes%2f%0a" target=_blank class="share-btn whatsapp"><svg class="widget-social__link-icon icon icon-whatsapp" width="24" height="24" viewBox="0 0 24 24"><path d="M20.1 3.9C17.9 1.7 15 .5 12 .5 5.8.5.7 5.6.7 11.9c0 2 .5 3.9 1.5 5.6L.6 23.4l6-1.6c1.6.9 3.5 1.3 5.4 1.3 6.3.0 11.4-5.1 11.4-11.4-.1-2.8-1.2-5.7-3.3-7.8zM12 21.4c-1.7.0-3.3-.5-4.8-1.3l-.4-.2-3.5 1 1-3.4L4 17c-1-1.5-1.4-3.2-1.4-5.1.0-5.2 4.2-9.4 9.4-9.4 2.5.0 4.9 1 6.7 2.8s2.8 4.2 2.8 6.7c-.1 5.2-4.3 9.4-9.5 9.4zm5.1-7.1c-.3-.1-1.7-.9-1.9-1-.3-.1-.5-.1-.7.1-.2.3-.8 1-.9 1.1-.2.2-.3.2-.6.1s-1.2-.5-2.3-1.4c-.9-.8-1.4-1.7-1.6-2s0-.5.1-.6.3-.3.4-.5c.2-.1.3-.3.4-.5s0-.4.0-.5C10 9 9.3 7.6 9 7c-.1-.4-.4-.3-.5-.3h-.6s-.4.1-.7.3c-.3.3-1 1-1 2.4s1 2.8 1.1 3c.1.2 2 3.1 4.9 4.3.7.3 1.2.5 1.6.6.7.2 1.3.2 1.8.1.6-.1 1.7-.7 1.9-1.3.2-.7.2-1.2.2-1.3-.1-.3-.3-.4-.6-.5z"/></svg><p>Email</p></a></li><li><a href="mailto:?subject=Coding%20N%20Concepts - Apache%20Kafka%20CCDAK%20Exam%20Notes.&body=Apache%20Kafka%20CCDAK%20Exam%20Notes%2c%20by%20Coding%20N%20Concepts%0aStudy%20guide%20for%20Kafka%20Certification%20CCDAK%20%28Certified%20Developer%20for%20Apache%20Kafka%29%20and%20CCOAK%20%28Certified%20Operator%20for%20Apache%20Kafka%29%20preparation%20with%20sample%20exam%20questions.%0a%0ahttps%3a%2f%2fcodingnconcepts.com%2fpost%2fapache-kafka-ccdak-exam-notes%2f%0a" target=_blank class="share-btn email"><svg class="widget-social__link-icon icon icon-mail" width="24" height="24" viewBox="0 0 416 288"><path d="m0 16v256 16h16 384 16v-16V16 0h-16H16 0zm347 16-139 92.5L69 32zM199 157.5l9 5.5 9-5.5L384 46v210H32V46z"/></svg><p>Email</p></a></li></ul></section><div class="authorbox clearfix"><figure class=authorbox__avatar><img alt="Ashish Lahoti avatar" src=/media/authors/ashishlahoti.png class=avatar height=90 width=90></figure><div class=authorbox__header><span class=authorbox__name>About Ashish Lahoti</span></div><div class=authorbox__description>Ashish Lahoti is a senior application developer at DBS Bank having 10+ years of experience in full stack technologies | Confluent Certified Developer for Apache KAFKA | SCJP Certified</div></div><nav class="post-nav flex"><div class="post-nav__item post-nav__item--prev"><a class=post-nav__link href=/post/apache-kafka-consumer-using-java/ rel=prev><span class=post-nav__caption>«&#8201;Previous</span><p class=post-nav__post-title>Kafka Consumer Using Java</p></a></div><div class="post-nav__item post-nav__item--next"><a class=post-nav__link href=/post/blockchain-explained/ rel=next><span class=post-nav__caption>Next&#8201;»</span><p class=post-nav__post-title>Blockchain Explained</p></a></div></nav><section class=comments><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"codingnconcepts"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 CodingNConcepts.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script></body></html>